- ++train_config.enable_fsdp=false
- ++train_config.enable_ddp=true
- ++fsdp_config.pure_bf16=true
- ++log_config.use_wandb=true
- ++log_config.wandb_project_name=trans_gemma12b
- ++log_config.wandb_exp_name=yxduir
- ++train_config.validation_interval=2000
- ++log_config.wandb_exp_name=srt
- ++train_config.use_peft=false
- ++model_config.llm_name=translategemma-12b-it
- ++model_config.llm_path=/mgData2/yxdu/m2m-70/SLAM-LLM/../models/translategemma-12b-it
- ++model_config.llm_dim=3840
- ++model_config.encoder_name=whisper
- ++model_config.encoder_projector_ds_rate=5
- ++model_config.encoder_path=
- ++model_config.encoder_path_hf=/mgData2/yxdu/m2m-70/SLAM-LLM/../models/whisper-large-v3
- ++model_config.encoder_dim=1280
- ++model_config.encoder_projector=qqm
- ++model_config.query_len=150
- ++dataset_config.dataset=st_dataset
- ++dataset_config.file=examples/st_covost2/dataset/st_dataset.py:get_speech_dataset
- ++dataset_config.train_data_path=/mgData2/yxdu/data/fleurs_all/data/srt_train_300_50.jsonl
- ++dataset_config.val_data_path=/mgData2/yxdu/data/fleurs_all/data/srt_test_10_50.jsonl
- ++dataset_config.input_type=mel
- ++dataset_config.mel_size=128
- ++dataset_config.fix_length_audio=30
- ++dataset_config.source=all
- ++dataset_config.mode=srt
- ++train_config.model_name=asr
- ++train_config.num_epochs=20
- ++train_config.freeze_encoder=true
- ++train_config.freeze_llm=true
- ++train_config.batching_strategy=custom
- ++train_config.gradient_accumulation_steps=1
- ++train_config.warmup_steps=1000
- ++train_config.total_steps=200000
- ++train_config.lr=1e-4
- ++train_config.batch_size_training=8
- ++train_config.val_batch_size=16
- ++train_config.num_workers_dataloader=16
- ++train_config.output_dir=/mgData2/yxdu/m2m-70/SLAM-LLM/output/s2tt_gemma12b_srt_50
- ++metric=acc
- ++train_config.use_fp16=false
- ++dataset_config.validnum=-2
- ++train_config.use_fast_kernels=false
- ++ckpt_path=/mgData2/yxdu/m2m-70/SLAM-LLM/output/s2tt_gemma12b_asr-70-fleurs/asr_epoch_8_step_297/model.pt
